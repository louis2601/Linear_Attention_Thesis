{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-04T13:32:29.069016Z",
     "start_time": "2025-12-04T13:32:26.138016Z"
    }
   },
   "source": "pip install torch transformers bitsandbytes peft trl datasets accelerate scipy",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (5.0.0.dev0)\n",
      "Requirement already satisfied: bitsandbytes in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.48.2)\n",
      "Requirement already satisfied: peft in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: trl in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: datasets in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: filelock in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: typer-slim in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.0.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: anyio in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.0.0->transformers) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\users\\louis\\pycharmprojects\\master_thesis\\babilong_benchmark\\.venv\\lib\\site-packages (from typer-slim->transformers) (8.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:27:36.704835Z",
     "start_time": "2025-12-08T18:26:54.407191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from source.babilong.prompts import DEFAULT_PROMPTS, DEFAULT_TEMPLATE, get_formatted_input\n",
    "from source.babilong.babilong_utils import compare_answers"
   ],
   "id": "3843962c81971efa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup and Config",
   "id": "bd4351a493902b1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:28:05.760040Z",
     "start_time": "2025-12-08T18:27:36.704835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_ID = \"state-spaces/mamba-1.4b-hf\"\n",
    "OUTPUT_DIR = \"./babilong_mamba_finetune\"\n",
    "\n",
    "# Task Setup\n",
    "TASK_NAME = \"qa1\"\n",
    "SPLIT_LENGTH = \"0k\"  # Ideal f√ºr den Start auf der 3080\n",
    "\n",
    "# QLoRA Config (Speicher sparen)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # RTX 3080 Feature\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    device_map = {\"\": torch.cuda.current_device()}\n",
    "else:\n",
    "    device_map = \"auto\" # Fallback\n",
    "\n",
    "# 2. Modell laden mit der Config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=device_map,  # <--- CHANGED from \"auto\" to explicit variable\n",
    "    trust_remote_code=True  # Mamba braucht das oft noch\n",
    ")\n",
    "\n",
    "print(f\"Model successfully loaded on device: {device_map}\")"
   ],
   "id": "e43fb9ba9511672b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "513c7f68f9be42c2a09a9b73b902cd38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded on device: {'': 0}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T18:28:34.850031Z",
     "start_time": "2025-12-08T18:28:34.843050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LoRA Config f√ºr MAMBA\n",
    "# Mamba hat spezifische Layer-Namen. 'all-linear' ist hier der sicherste Weg,\n",
    "# um 'in_proj', 'out_proj', 'x_proj' und 'dt_proj' automatisch zu erwischen.\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"in_proj\", \"x_proj\", \"dt_proj\"]\n",
    ")"
   ],
   "id": "a158b8104dfe7fe1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:06.673996Z",
     "start_time": "2025-12-04T21:50:05.969274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"state-spaces/mamba-1.4b-hf\"\n",
    "\n",
    "print(f\"Lade Tokenizer f√ºr {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Fix f√ºr Mamba / GPT-NeoX Tokenizer:\n",
    "# Da kein 'pad_token' definiert ist, nutzen wir das End-of-Sentence Token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# F√ºr Training mit SFTTrainer ist \"right\" padding Standard\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "print(f\"Tokenizer geladen. Pad Token ID: {tokenizer.pad_token_id}\")"
   ],
   "id": "a59ab2010b4b6e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Tokenizer f√ºr state-spaces/mamba-1.4b-hf...\n",
      "Tokenizer geladen. Pad Token ID: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "3b7649b802146d21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:14.335341Z",
     "start_time": "2025-12-04T21:50:08.591663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ==========================================\n",
    "# DATASET LADEN & FORMATIEREN\n",
    "# ==========================================\n",
    "\n",
    "print(f\"Lade BABILong {TASK_NAME} ({SPLIT_LENGTH})...\")\n",
    "\n",
    "# 1. Laden des spezifischen Splits (z.B. split='qa1')\n",
    "# Das l√§dt NUR die Daten f√ºr diesen Task. Wir m√ºssen nicht mehr filtern.\n",
    "dataset = load_dataset(\"RMT-team/babilong\", SPLIT_LENGTH, split=TASK_NAME)\n",
    "\n",
    "# DEBUG: Zeige uns, welche Spalten wirklich da sind (vermeidet KeyErrors in der Zukunft)\n",
    "print(f\"Verf√ºgbare Spalten: {dataset.column_names}\")\n",
    "\n",
    "# 2. Optional: Nur einen kleinen Teil zum Testen nutzen (Auskommentieren f√ºr echtes Training)\n",
    "# dataset = dataset.select(range(100)) \n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"\n",
    "    Formatierungsfunktion V3 (Final Fix).\n",
    "    Unterscheidet sauber zwischen Batch (Liste) und Single (String),\n",
    "    damit der SFTTrainer nicht √ºber Datentypen stolpert.\n",
    "    \"\"\"\n",
    "    # 1. Pr√ºfen: Haben wir einen Batch (Liste von Inputs) oder ein einzelnes Item?\n",
    "    # Wir pr√ºfen 'input', da dies im Dataset vorhanden ist.\n",
    "    is_batch = isinstance(example['input'], list)\n",
    "    \n",
    "    if is_batch:\n",
    "        # === BATCH MODUS ===\n",
    "        output_texts = []\n",
    "        for i in range(len(example['input'])):\n",
    "            text = (\n",
    "                f\"Context: {example['input'][i]}\\n\\n\"\n",
    "                f\"Question: {example['question'][i]}\\n\\n\"\n",
    "                f\"Answer: {example['target'][i]}\"\n",
    "            )\n",
    "            output_texts.append(text)\n",
    "        return output_texts # R√ºckgabe: Liste von Strings\n",
    "        \n",
    "    else:\n",
    "        # === SINGLE SAMPLE MODUS ===\n",
    "        # Hier d√ºrfen wir KEINE Liste zur√ºckgeben, sondern nur den nackten String!\n",
    "        text = (\n",
    "            f\"Context: {example['input']}\\n\\n\"\n",
    "            f\"Question: {example['question']}\\n\\n\"\n",
    "            f\"Answer: {example['target']}\"\n",
    "        )\n",
    "        return text # R√ºckgabe: Ein einzelner String\n",
    "\n",
    "print(\"Dataset erfolgreich geladen und bereit.\")"
   ],
   "id": "8048b40bf7106148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade BABILong qa1 (0k)...\n",
      "Verf√ºgbare Spalten: ['target', 'input', 'question']\n",
      "Dataset erfolgreich geladen und bereit.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:14.340331Z",
     "start_time": "2025-12-04T21:50:14.336341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trigger_text = \"\\nAnswer: The most recent location of\"\n",
    "task = 'qa1'\n",
    "use_chat_template = False\n",
    "use_instruction = True\n",
    "use_examples = True\n",
    "use_post_prompt = True\n",
    "# 2. Update your prompt_cfg\n",
    "prompt_cfg = {\n",
    "'instruction': DEFAULT_PROMPTS[task]['instruction'] if use_instruction else '',\n",
    "'examples': DEFAULT_PROMPTS[task]['examples'] if use_examples else '',\n",
    "'post_prompt': DEFAULT_PROMPTS[task]['post_prompt'] if use_post_prompt else '',\n",
    "\n",
    "# CRITICAL CHANGE: Append the trigger to the template itself\n",
    "# Old: \"{instruction}... Question: {question}\"\n",
    "# New: \"{instruction}... Question: {question}\\nAnswer: The most recent location of\"\n",
    "'template': DEFAULT_TEMPLATE + trigger_text, \n",
    "\n",
    "'chat_template': use_chat_template,\n",
    "}"
   ],
   "id": "d4890389b7407920",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:14.425612Z",
     "start_time": "2025-12-04T21:50:14.340834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP CONFIG (Same as before)\n",
    "# ==========================================\n",
    "trigger_phrase = \"The most recent location of\"\n",
    "trigger_text = f\"\\nAnswer: {trigger_phrase}\"\n",
    "\n",
    "# Ensure prompt config is ready\n",
    "prompt_cfg = {\n",
    "    'instruction': DEFAULT_PROMPTS[task]['instruction'] if use_instruction else '',\n",
    "    'examples': DEFAULT_PROMPTS[task]['examples'] if use_examples else '',\n",
    "    'post_prompt': DEFAULT_PROMPTS[task]['post_prompt'] if use_post_prompt else '',\n",
    "    'template': DEFAULT_TEMPLATE + trigger_text, \n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 2. INTELLIGENT MAPPING FUNCTION\n",
    "# ==========================================\n",
    "def format_aligned_training(example):\n",
    "    # A. Generate Prompt (Ends with \"...location of\")\n",
    "    prompt_str = get_formatted_input(\n",
    "        context=example['input'], \n",
    "        question=example['question'], \n",
    "        examples=prompt_cfg['examples'],\n",
    "        instruction=prompt_cfg['instruction'], \n",
    "        post_prompt=prompt_cfg['post_prompt'],\n",
    "        template=prompt_cfg['template']\n",
    "    )\n",
    "    \n",
    "    # B. Generate the CORRECT Completion\n",
    "    target_loc = example['target']      # e.g., \"bathroom\"\n",
    "    question_str = example['question']  # e.g., \"Where is Mary?\"\n",
    "    \n",
    "    # 1. Extract the Person's Name from the Question\n",
    "    # QA1 questions are always \"Where is [Name]?\"\n",
    "    # We strip \"Where is \" and the \"?\"\n",
    "    match = re.search(r\"Where is (.*?)\\?\", question_str)\n",
    "    \n",
    "    if match:\n",
    "        person_name = match.group(1) # e.g., \"Mary\"\n",
    "        \n",
    "        # 2. Construct the missing bridge\n",
    "        # We need: \" Mary is \"\n",
    "        bridge = f\" {person_name} is\"\n",
    "        \n",
    "        # 3. Combine: \" Mary is bathroom.\"\n",
    "        # Note: BAbI grammar is usually \"Mary is bathroom\", not \"Mary is in the bathroom\"\n",
    "        completion_str = f\"{bridge} {target_loc}\" \n",
    "        \n",
    "    else:\n",
    "        # Fallback if regex fails (shouldn't happen on QA1)\n",
    "        completion_str = f\" {target_loc}\"\n",
    "\n",
    "    # Add EOS\n",
    "    completion_str = f\"{completion_str}{tokenizer.eos_token}\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt_str,\n",
    "        \"completion\": completion_str\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 3. VERIFY AGAIN\n",
    "# ==========================================\n",
    "print(\"‚è≥ Remapping with Grammar Fix...\")\n",
    "aligned_dataset = dataset.map(format_aligned_training, remove_columns=dataset.column_names)\n",
    "\n",
    "print(\"\\n--- [PROMPT END] ---\")\n",
    "print(f\"...{aligned_dataset[0]['prompt'][-50:]}\")\n",
    "\n",
    "print(f\"\\n--- [COMPLETION] ---\")\n",
    "print(f\"'{aligned_dataset[0]['completion']}'\")"
   ],
   "id": "f73be9188eb552c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Remapping with Grammar Fix...\n",
      "\n",
      "--- [PROMPT END] ---\n",
      "...here is Mary? \n",
      "Answer: The most recent location of\n",
      "\n",
      "--- [COMPLETION] ---\n",
      "' Mary is bathroom<|endoftext|>'\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:15.944744Z",
     "start_time": "2025-12-04T21:50:15.902820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"‚è≥ Mapping dataset...\")\n",
    "aligned_dataset = dataset.map(format_aligned_training, remove_columns=dataset.column_names)\n",
    "\n",
    "print(\"\\n‚úÖ Mapping Complete. Verifying one sample:\")\n",
    "sample = aligned_dataset[0]\n",
    "\n",
    "print(f\"\\n--- [PROMPT END (Last 100 chars)] ---\")\n",
    "print(f\"...{sample['prompt'][-100:]}\")\n",
    "\n",
    "print(f\"\\n--- [COMPLETION (Target)] ---\")\n",
    "print(f\"'{sample['completion']}'\")\n",
    "\n",
    "# LOGIC CHECK\n",
    "if sample['prompt'].endswith(trigger_phrase) and not sample['completion'].strip().startswith(\"The most\"):\n",
    "    print(\"\\nSUCCESS: Trigger is in prompt and removed from completion.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Check the output above. You might have double text.\")"
   ],
   "id": "c8ff067cbc2ca5d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Mapping dataset...\n",
      "\n",
      "‚úÖ Mapping Complete. Verifying one sample:\n",
      "\n",
      "--- [PROMPT END (Last 100 chars)] ---\n",
      "...John moved to the bedroom.\n",
      "</context>\n",
      "\n",
      "Question: Where is Mary? \n",
      "Answer: The most recent location of\n",
      "\n",
      "--- [COMPLETION (Target)] ---\n",
      "' Mary is bathroom<|endoftext|>'\n",
      "\n",
      "SUCCESS: Trigger is in prompt and removed from completion.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:18.612642Z",
     "start_time": "2025-12-04T21:50:18.550612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Split the dataset\n",
    "# seed=42 ensures the split is the same every time you run it (reproducibility)\n",
    "split_dataset = aligned_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split_dataset[\"train\"] # 90 samples\n",
    "eval_dataset = split_dataset[\"test\"]"
   ],
   "id": "d7216ca2458f9c55",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:20.326184Z",
     "start_time": "2025-12-04T21:50:20.320026Z"
    }
   },
   "cell_type": "code",
   "source": "formatting_prompts_func(dataset[0])",
   "id": "1ce639c8f0d02ec9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context: John travelled to the hallway. Mary journeyed to the bathroom. Daniel went back to the bathroom. John moved to the bedroom.\\n\\nQuestion: Where is Mary? \\n\\nAnswer: bathroom'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T18:39:07.806024Z",
     "start_time": "2025-12-04T18:39:07.800065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Take the first sample of the dataset\n",
    "first_sample = dataset[0]\n",
    "\n",
    "# If you want to overwrite the dataset with just the first sample\n",
    "dataset = dataset.select([0])"
   ],
   "id": "77962fbd68bb30b5",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:50:22.719305Z",
     "start_time": "2025-12-04T21:50:22.714458Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset[0]",
   "id": "2520242a70fcdf41",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'I will give you context with the facts about positions of different persons hidden in some random text and a question. You need to answer the question based only on the information from the facts. If a person was in different locations, use the latest location to answer the question.\\n\\n<example>\\nCharlie went to the hallway. Judith come back to the kitchen. Charlie travelled to balcony. Where is Charlie?\\nAnswer: The most recent location of Charlie is balcony.\\n</example>\\n\\n<example>\\nAlan moved to the garage. Charlie went to the beach. Alan went to the shop. Rouse travelled to balcony. Where is Alan?\\nAnswer: The most recent location of Alan is shop.\\n</example>\\n\\nAlways return your answer in the following format: The most recent location of ‚Äôperson‚Äô is ‚Äôlocation‚Äô. Do not write anything else after that.\\n\\n<context>\\nJohn travelled to the office. Mary journeyed to the kitchen.\\n</context>\\n\\nQuestion: Where is Mary? \\nAnswer: The most recent location of',\n",
       " 'completion': ' Mary is kitchen<|endoftext|>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T21:44:36.580189Z",
     "start_time": "2025-12-04T21:44:36.576943Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"hi\")",
   "id": "cddfe12f29fb6e07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Config and start",
   "id": "3d3775ddf30e65a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:54:12.444550Z",
     "start_time": "2025-12-04T21:50:24.820466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, MambaForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# 3. ENSURE DATA IS SPLIT\n",
    "# We assume 'aligned_dataset' exists from the previous step\n",
    "if \"test\" not in aligned_dataset:\n",
    "    print(\"‚úÇÔ∏è Splitting dataset...\")\n",
    "    split_dataset = aligned_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = aligned_dataset[\"train\"]\n",
    "    eval_dataset = aligned_dataset[\"test\"]\n",
    "\n",
    "# 4. CONFIGURATION\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    # Training Params\n",
    "    num_train_epochs=20,             \n",
    "    eval_strategy=\"epoch\",           \n",
    "    save_strategy=\"epoch\",           \n",
    "    load_best_model_at_end=True,     \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Masking settings\n",
    "    completion_only_loss=True,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    packing=False,\n",
    "    \n",
    "    # Optimization\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=False,\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "print(\"Initialisiere SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,      \n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "# 5. RUN\n",
    "print(\"üöÄ Starte Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "print(f\"\\nTraining beendet. Speichere Adapter in {OUTPUT_DIR}...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ],
   "id": "6b3093604c6d52af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Splitting dataset...\n",
      "Initialisiere SFTTrainer...\n",
      "üöÄ Starte Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 14:01:37, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.235423</td>\n",
       "      <td>2.502059</td>\n",
       "      <td>22291.000000</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.127524</td>\n",
       "      <td>2.317039</td>\n",
       "      <td>44582.000000</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.101663</td>\n",
       "      <td>2.242300</td>\n",
       "      <td>66873.000000</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.028360</td>\n",
       "      <td>2.200406</td>\n",
       "      <td>89164.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.029991</td>\n",
       "      <td>2.174889</td>\n",
       "      <td>111455.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.029384</td>\n",
       "      <td>2.163837</td>\n",
       "      <td>133746.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.029215</td>\n",
       "      <td>2.154321</td>\n",
       "      <td>156037.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.033487</td>\n",
       "      <td>2.150412</td>\n",
       "      <td>178328.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>2.145832</td>\n",
       "      <td>200619.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.033371</td>\n",
       "      <td>2.141828</td>\n",
       "      <td>222910.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033328</td>\n",
       "      <td>2.138382</td>\n",
       "      <td>245201.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>2.134026</td>\n",
       "      <td>267492.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033277</td>\n",
       "      <td>2.132065</td>\n",
       "      <td>289783.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033288</td>\n",
       "      <td>2.131095</td>\n",
       "      <td>312074.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.037944</td>\n",
       "      <td>2.131198</td>\n",
       "      <td>334365.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>2.128946</td>\n",
       "      <td>356656.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>2.124013</td>\n",
       "      <td>378947.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.028819</td>\n",
       "      <td>2.123187</td>\n",
       "      <td>401238.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033240</td>\n",
       "      <td>2.127030</td>\n",
       "      <td>423529.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>2.125100</td>\n",
       "      <td>445820.000000</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "D:\\Users\\Louis\\PycharmProjects\\Master_thesis\\Babilong_Benchmark\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training beendet. Speichere Adapter in ./babilong_mamba_finetune...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./babilong_mamba_finetune\\\\tokenizer_config.json',\n",
       " './babilong_mamba_finetune\\\\special_tokens_map.json',\n",
       " './babilong_mamba_finetune\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T18:40:14.282596Z",
     "start_time": "2025-12-04T18:40:14.279605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "0,# Check if the model already has a `peft_config` attribute\n",
    "if hasattr(model, \"peft_config\"):\n",
    "    print(\"Warning: The model already has a `peft_config` attribute. Removing it to avoid multiple adapters.\")\n",
    "    del model.peft_config  # Remove the existing `peft_config`\n",
    "else:\n",
    "    print(\"No `peft_config` attribute found in the model. Safe to proceed.\")\n"
   ],
   "id": "d4aa5962593b8c9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No `peft_config` attribute found in the model. Safe to proceed.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2aefb3d3be4cedd2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
